{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Chào bạn\\! Rất vui được tiếp tục hỗ trợ bạn.\n",
        "\n",
        "Trong LangChain, khi làm việc với `ChatOpenAI` (hoặc các lớp LLM tương tự), tham số dùng để truyền **input** vào mô hình không phải là một tham số riêng biệt trong hàm khởi tạo (`__init__`) của lớp `ChatOpenAI` như `max_completion_tokens`. Thay vào đó, input được truyền vào khi bạn **gọi phương thức `invoke()`** (hoặc `predict()`, `__call__()` tùy phiên bản và mục đích sử dụng) của đối tượng `ChatOpenAI`.\n",
        "\n",
        "Cụ thể, input cho các mô hình chat (như GPT-4, GPT-3.5-turbo) thường là một danh sách các **`Message` object**. Các loại `Message` phổ biến nhất là:\n",
        "\n",
        "  * **`HumanMessage`**: Tin nhắn từ người dùng.\n",
        "  * **`AIMessage`**: Tin nhắn từ mô hình AI (phản hồi trước đó).\n",
        "  * **`SystemMessage`**: Tin nhắn cấu hình hành vi của AI (ví dụ: \"Bạn là một trợ lý hữu ích...\").\n",
        "  * **`ToolMessage`**: Tin nhắn chứa kết quả từ việc gọi một công cụ.\n",
        "  * **`FunctionMessage`**: Tương tự `ToolMessage`, dùng khi mô hình gọi một hàm.\n",
        "\n",
        "-----\n",
        "\n",
        "### Cách truyền Input vào `ChatOpenAI`\n",
        "\n",
        "Dưới đây là ví dụ minh họa cách bạn truyền input (là một danh sách các `Message`) vào mô hình:"
      ],
      "metadata": {
        "id": "MvDrFj0KuAG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
        "\n",
        "# Khởi tạo mô hình ChatOpenAI\n",
        "# Các tham số này cấu hình hành vi chung của mô hình, không phải input\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o\",\n",
        "    temperature=0.7,\n",
        "    max_completion_tokens=200, # Ví dụ: giới hạn output\n",
        ")\n",
        "\n",
        "# 1. Input đơn giản: Chỉ có HumanMessage\n",
        "print(\"--- Ví dụ 1: Input chỉ có HumanMessage ---\")\n",
        "simple_messages = [\n",
        "    HumanMessage(content=\"Kể một câu chuyện cười.\")\n",
        "]\n",
        "response_simple = llm.invoke(simple_messages)\n",
        "print(f\"Human: {simple_messages[0].content}\")\n",
        "print(f\"AI: {response_simple.content}\\n\")\n",
        "\n",
        "# 2. Input phức tạp hơn: Sử dụng SystemMessage và luồng hội thoại\n",
        "print(\"--- Ví dụ 2: Luồng hội thoại với SystemMessage ---\")\n",
        "conversation_messages = [\n",
        "    SystemMessage(content=\"Bạn là một trợ lý về lịch sử Việt Nam, cung cấp thông tin ngắn gọn và chính xác.\"),\n",
        "    HumanMessage(content=\"Ai là người lãnh đạo cuộc khởi nghĩa Lam Sơn?\"),\n",
        "]\n",
        "response_1 = llm.invoke(conversation_messages)\n",
        "print(f\"System: {conversation_messages[0].content}\")\n",
        "print(f\"Human: {conversation_messages[1].content}\")\n",
        "print(f\"AI: {response_1.content}\\n\")\n",
        "\n",
        "# Thêm phản hồi của AI vào cuộc hội thoại để duy trì ngữ cảnh\n",
        "conversation_messages.append(AIMessage(content=response_1.content))\n",
        "conversation_messages.append(HumanMessage(content=\"Cuộc khởi nghĩa đó kết thúc vào năm nào?\"))\n",
        "\n",
        "response_2 = llm.invoke(conversation_messages)\n",
        "print(f\"Human: {conversation_messages[-1].content}\") # Tin nhắn mới nhất\n",
        "print(f\"AI: {response_2.content}\\n\")\n",
        "\n",
        "# 3. Input với chuỗi (String):\n",
        "# Bạn cũng có thể truyền trực tiếp một chuỗi (string) làm input.\n",
        "# LangChain sẽ tự động chuyển đổi chuỗi đó thành một HumanMessage.\n",
        "print(\"--- Ví dụ 3: Input là chuỗi (string) ---\")\n",
        "string_input_response = llm.invoke(\"Hà Nội là thủ đô của nước nào?\")\n",
        "print(f\"Human: Hà Nội là thủ đô của nước nào?\")\n",
        "print(f\"AI: {string_input_response.content}\\n\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "VrJsMR9XuAHD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "### Giải thích và Best Practices\n",
        "\n",
        "  * **Danh sách `Message`**: Đây là cách tiêu chuẩn và mạnh mẽ nhất để cung cấp input cho các mô hình chat. Nó cho phép bạn xây dựng các cuộc hội thoại phức tạp, định rõ vai trò (system, user, assistant), và truyền tải nhiều loại thông tin khác nhau.\n",
        "  * **Duy trì ngữ cảnh**: Để mô hình có thể \"nhớ\" được các lượt trò chuyện trước đó, bạn cần phải tự mình **duy trì danh sách các `Message`** và truyền toàn bộ danh sách đó (bao gồm cả các tin nhắn cũ và tin nhắn mới) trong mỗi lần gọi `invoke()`. LangChain không tự động lưu trữ lịch sử hội thoại cho bạn ở cấp độ `ChatOpenAI`.\n",
        "  * **`SystemMessage`**: Sử dụng `SystemMessage` là một best practice để \"hướng dẫn\" hoặc \"định hướng\" hành vi của mô hình. Bạn có thể sử dụng nó để chỉ định vai trò của AI, giới hạn phong cách trả lời, hoặc đặt ra các nguyên tắc.\n",
        "  * **Token Context Window**: Hãy luôn nhớ rằng các mô hình LLM có giới hạn về số lượng token có thể xử lý trong một lần gọi (gọi là \"context window\"). Nếu cuộc hội thoại của bạn quá dài, bạn có thể cần phải triển khai các chiến lược như **tóm tắt (summarization)** hoặc **đệm (truncation)** lịch sử hội thoại để không vượt quá giới hạn này, tránh lỗi hoặc làm giảm hiệu suất.\n",
        "  * **Chuỗi (String) làm Input**: Mặc dù tiện lợi cho các trường hợp đơn giản, việc chỉ truyền một chuỗi sẽ không cho phép bạn kiểm soát vai trò của các tin nhắn (ví dụ: không có `SystemMessage` hay `AIMessage` trước đó) và không lý tưởng cho các cuộc hội thoại đa lượt.\n",
        "\n",
        "Hy vọng giải thích này giúp bạn hiểu rõ hơn về cách truyền input trong LangChain\\! Bạn còn muốn tìm hiểu thêm về tham số nào khác không?"
      ],
      "metadata": {
        "id": "jPtVklARuAHF"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}