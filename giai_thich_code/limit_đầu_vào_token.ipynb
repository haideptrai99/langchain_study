{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Bạn có một câu hỏi rất hay và quan trọng\\! Trong khi `max_completion_tokens` (hoặc `max_tokens` cũ) giới hạn số lượng token đầu ra do mô hình sinh ra, thì **không có một tham số trực tiếp nào trong `ChatOpenAI` của LangChain cho phép bạn \"limit\" số lượng token đầu vào**.\n",
        "\n",
        "Thay vào đó, việc quản lý giới hạn token đầu vào (input tokens) nằm ở **mô hình LLM bạn đang sử dụng và cách bạn xây dựng `messages` (input) của mình**.\n",
        "\n",
        "Dưới đây là giải thích chi tiết:\n",
        "\n",
        "### 1\\. Giới hạn Token của Mô hình (Model's Context Window)\n",
        "\n",
        "Mỗi mô hình LLM (như GPT-4o, GPT-4, GPT-3.5-turbo của OpenAI) đều có một **giới hạn ngữ cảnh (context window)** tối đa. Giới hạn này bao gồm **tổng số token của cả input (prompt) và output (completion)**.\n",
        "\n",
        "Ví dụ:\n",
        "\n",
        "  * `gpt-4o`: có thể lên tới 128,000 token (input + output).\n",
        "  * `gpt-3.5-turbo`: thường có 16,385 token hoặc thấp hơn tùy phiên bản.\n",
        "\n",
        "Nếu tổng số token của `messages` bạn truyền vào và số token dự kiến mà mô hình sẽ tạo ra vượt quá giới hạn ngữ cảnh này, API của OpenAI sẽ trả về lỗi `context_length_exceeded`.\n",
        "\n",
        "### 2\\. Cách LangChain và OpenAI API xử lý Input Tokens\n",
        "\n",
        "  * **Không có tham số `max_input_tokens` trực tiếp:** LangChain và bản thân OpenAI API không cung cấp một tham số để bạn nói \"chỉ sử dụng tối đa X token từ input\". Điều này là do mô hình cần đọc toàn bộ input để hiểu ngữ cảnh và tạo ra phản hồi phù hợp.\n",
        "  * **Bạn phải quản lý input:** Trách nhiệm của bạn là đảm bảo rằng tổng số token trong danh sách `messages` bạn cung cấp cho `llm.invoke()` **cộng với** `max_completion_tokens` (số token đầu ra tối đa bạn cho phép) **không vượt quá giới hạn ngữ cảnh của mô hình**.\n",
        "\n",
        "### 3\\. Chiến lược để quản lý Input Tokens\n",
        "\n",
        "Vì không có tham số trực tiếp, bạn cần chủ động quản lý độ dài của `messages` trước khi gửi chúng tới `ChatOpenAI`. Đây là một số chiến lược phổ biến:\n",
        "\n",
        "#### a. Đếm Token trước khi gửi\n",
        "\n",
        "Bạn có thể sử dụng thư viện `tiktoken` của OpenAI để đếm số token của các tin nhắn trước khi gửi chúng:"
      ],
      "metadata": {
        "id": "JybV3H4KwRum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
        "\n",
        "# Khởi tạo mô hình\n",
        "llm = ChatOpenAI(model=\"gpt-4o\") # GPT-4o có context window 128k\n",
        "tokenizer = tiktoken.encoding_for_model(\"gpt-4o\") # Chọn tokenizer phù hợp với mô hình\n",
        "\n",
        "def count_message_tokens(messages, model=\"gpt-4o\"):\n",
        "    \"\"\"\n",
        "    Đếm số token của một danh sách tin nhắn.\n",
        "    Dựa trên hướng dẫn của OpenAI về cách tính token cho chat completions.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        encoding = tiktoken.encoding_for_model(model)\n",
        "    except KeyError:\n",
        "        encoding = tiktoken.get_encoding(\"cl100k_base\") # Fallback cho các model không rõ\n",
        "\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        # Mỗi message có 4 token cho vai trò và nội dung\n",
        "        # (name, role, content, function_call nếu có)\n",
        "        # Cộng thêm 1 token cho trường hợp không có nội dung (ví dụ: tool_calls)\n",
        "        num_tokens += 4\n",
        "        if isinstance(message, HumanMessage) or \\\n",
        "           isinstance(message, SystemMessage) or \\\n",
        "           isinstance(message, AIMessage):\n",
        "            num_tokens += len(encoding.encode(message.content))\n",
        "        # Có thể thêm các loại message khác như ToolMessage, FunctionMessage\n",
        "        # và cách tính token tương ứng nếu bạn sử dụng chúng.\n",
        "    num_tokens += 2 # Mỗi response bắt đầu bằng assistant\n",
        "    return num_tokens\n",
        "\n",
        "# Ví dụ về các tin nhắn\n",
        "messages_long = [\n",
        "    SystemMessage(content=\"Bạn là một chuyên gia về lịch sử thế giới. Hãy cung cấp thông tin chi tiết và đầy đủ.\"),\n",
        "    HumanMessage(content=\"Hãy kể về tất cả các cuộc chiến tranh lớn trong lịch sử nhân loại từ thời cổ đại đến nay, bao gồm nguyên nhân, diễn biến chính và hậu quả, và các bên tham chiến. Phân tích chi tiết từng cuộc chiến và mối liên hệ giữa chúng. Mục tiêu là viết một bài luận lịch sử toàn diện.\")\n",
        "]\n",
        "\n",
        "# Thêm một tin nhắn cực dài để mô phỏng vượt quá giới hạn\n",
        "long_content = \" \".join([\"Đây là một đoạn văn bản rất dài.\" * 5000]) # 5000 * 8 words = 40000 words\n",
        "messages_long.append(HumanMessage(content=long_content))\n",
        "\n",
        "\n",
        "current_input_tokens = count_message_tokens(messages_long)\n",
        "print(f\"Tổng số token đầu vào dự kiến: {current_input_tokens}\")\n",
        "\n",
        "# Giả sử mô hình GPT-4o có context window là 128000\n",
        "# Và bạn muốn dành 500 token cho output (max_completion_tokens)\n",
        "model_max_context = 128000\n",
        "reserved_for_output = 500\n",
        "\n",
        "if current_input_tokens + reserved_for_output > model_max_context:\n",
        "    print(f\"Cảnh báo: Input tokens ({current_input_tokens}) + reserved output ({reserved_for_output}) \"\n",
        "          f\"vượt quá giới hạn ngữ cảnh của mô hình ({model_max_context}).\")\n",
        "    # Cần áp dụng chiến lược rút gọn\n",
        "    # Ví dụ đơn giản: chỉ giữ lại tin nhắn System và tin nhắn Human cuối cùng\n",
        "    # (Đây chỉ là ví dụ, trong thực tế cần logic thông minh hơn)\n",
        "    truncated_messages = [messages_long[0], messages_long[-1]]\n",
        "    current_input_tokens = count_message_tokens(truncated_messages)\n",
        "    print(f\"Tổng số token đầu vào sau khi rút gọn: {current_input_tokens}\")\n",
        "    messages_to_send = truncated_messages\n",
        "else:\n",
        "    print(\"Input tokens nằm trong giới hạn.\")\n",
        "    messages_to_send = messages_long\n",
        "\n",
        "# Gửi request\n",
        "try:\n",
        "    response = llm.invoke(messages_to_send, max_completion_tokens=reserved_for_output)\n",
        "    print(\"\\n--- Phản hồi từ mô hình ---\")\n",
        "    print(response.content)\n",
        "    if response.usage_metadata:\n",
        "        print(f\"Input tokens thực tế: {response.usage_metadata['input_tokens']}\")\n",
        "        print(f\"Output tokens thực tế: {response.usage_metadata['output_tokens']}\")\n",
        "        print(f\"Total tokens thực tế: {response.usage_metadata['total_tokens']}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nLỗi khi gọi API: {e}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "Mlki8PCWwRur"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lưu ý về `count_message_tokens`**: Hàm này cung cấp ước tính. Cách tính token của OpenAI có thể hơi phức tạp hơn một chút với các loại tin nhắn khác nhau và overhead của API, nhưng cách trên là một ước tính khá tốt. Bạn có thể tham khảo thêm tài liệu của OpenAI về cách tính token cho chat completions.\n",
        "\n",
        "#### b. Rút gọn lịch sử hội thoại (History Truncation)\n",
        "\n",
        "Nếu bạn xây dựng chatbot với lịch sử hội thoại dài, bạn cần một cơ chế để rút gọn lịch sử đó:\n",
        "\n",
        "  * **Fixed Window**: Chỉ giữ lại N tin nhắn gần nhất.\n",
        "  * **Token-based Window**: Giữ lại tin nhắn cho đến khi tổng số token đạt một ngưỡng nhất định (ví dụ: 80% giới hạn của mô hình, để dành chỗ cho input mới và output).\n",
        "  * **Summarization**: Tóm tắt các phần cũ của cuộc hội thoại thành một `SystemMessage` hoặc `AIMessage` ngắn gọn. Đây là cách tốt nhất để giữ ngữ cảnh mà không tốn quá nhiều token. LangChain có các kỹ thuật `Memory` như `ConversationSummaryBufferMemory` để hỗ trợ việc này.\n",
        "\n",
        "#### c. RAG (Retrieval Augmented Generation)\n",
        "\n",
        "Nếu input của bạn là một tài liệu hoặc kiến thức nền lớn, thay vì đưa toàn bộ vào prompt, bạn nên sử dụng RAG. Kỹ thuật này bao gồm:\n",
        "\n",
        "1.  **Lưu trữ kiến thức:** Chia nhỏ tài liệu thành các \"chunk\" nhỏ và tạo embeddings cho chúng, lưu trữ trong Vector Store (ví dụ: Chroma, FAISS, Pinecone).\n",
        "2.  **Truy vấn liên quan:** Khi người dùng đặt câu hỏi, sử dụng câu hỏi đó để truy vấn Vector Store, tìm ra các chunk tài liệu liên quan nhất.\n",
        "3.  **Thêm vào prompt:** Chỉ đưa các chunk tài liệu liên quan này (đã được rút gọn hoặc chọn lọc) vào prompt cùng với câu hỏi của người dùng. Điều này giúp giảm đáng kể số token đầu vào.\n",
        "\n",
        "### Kết luận\n",
        "\n",
        "Để \"limit đầu vào token\" cho `ChatOpenAI`, bạn không có một tham số trực tiếp. Thay vào đó, bạn cần:\n",
        "\n",
        "1.  **Hiểu giới hạn context window của mô hình.**\n",
        "2.  **Đếm token input của bạn** (sử dụng `tiktoken`).\n",
        "3.  **Triển khai logic của riêng bạn** để rút gọn hoặc chọn lọc input (lịch sử hội thoại, tài liệu nguồn) nếu nó vượt quá giới hạn an toàn, để dành đủ chỗ cho phản hồi của mô hình.\n",
        "\n",
        "Việc quản lý token input hiệu quả là một phần quan trọng để tối ưu hóa chi phí và hiệu suất khi làm việc với các mô hình LLM."
      ],
      "metadata": {
        "id": "zPmK5V0VwRut"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}