{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Chào bạn\\! Với kinh nghiệm của một Full stack developer và AI engineer, đặc biệt là với LangChain và Hugging Face, tôi rất vui được hỗ trợ bạn.\n",
        "\n",
        "Hiện tại, trong class `ChatOpenAI` của LangChain, **tham số để giới hạn số lượng token đầu ra (completion tokens) đã được chuyển từ `max_tokens` sang `max_completion_tokens`**.\n",
        "\n",
        "Tuy nhiên, có một số điểm quan trọng cần lưu ý:\n",
        "\n",
        "  * **OpenAI API:** Bản thân OpenAI API vẫn hỗ trợ `max_tokens` cho các mô hình hiện có. Tuy nhiên, với các mô hình mới hơn (ví dụ: dòng mô hình `o1` của OpenAI), họ đã giới thiệu `max_completion_tokens` để làm rõ hơn rằng tham số này chỉ giới hạn số lượng token được **sinh ra** bởi mô hình (completion tokens), chứ không phải tổng số token bao gồm cả input.\n",
        "  * **LangChain và `ChatOpenAI`:** LangChain đã cập nhật để tương thích với thay đổi này. Khi bạn sử dụng `max_tokens` trong `ChatOpenAI`, LangChain sẽ tự động ánh xạ giá trị đó sang `max_completion_tokens`. Mặc dù `max_tokens` vẫn có thể hoạt động trong một số trường hợp, **khuyến nghị là bạn nên sử dụng `max_completion_tokens`** để đảm bảo tính rõ ràng và tương thích tốt nhất với các phiên bản mới của cả LangChain và OpenAI API.\n",
        "\n",
        "**Tóm lại:**\n",
        "\n",
        "  * **Nên dùng:** `max_completion_tokens`\n",
        "  * **Vẫn có thể hoạt động (nhưng không khuyến khích):** `max_tokens` (giá trị sẽ được LangChain tự động chuyển đổi).\n",
        "\n",
        "**Ví dụ:**"
      ],
      "metadata": {
        "id": "L0_JsG6tul9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "# Cách khuyến nghị\n",
        "llm_recommended = ChatOpenAI(\n",
        "    model=\"gpt-4o\",  # Hoặc bất kỳ mô hình nào bạn muốn sử dụng\n",
        "    temperature=0.7,\n",
        "    max_completion_tokens=150,  # Giới hạn 150 token cho phần phản hồi\n",
        ")\n",
        "\n",
        "# Cách vẫn có thể hoạt động (nhưng không khuyến nghị cho tương lai)\n",
        "llm_deprecated = ChatOpenAI(\n",
        "    model=\"gpt-4o\",\n",
        "    temperature=0.7,\n",
        "    max_tokens=150,  # LangChain sẽ xử lý nó thành max_completion_tokens\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"Bạn là một trợ lý hữu ích.\"),\n",
        "    HumanMessage(content=\"Hãy viết một đoạn văn ngắn về lợi ích của trí tuệ nhân tạo.\"),\n",
        "]\n",
        "\n",
        "print(\"--- Sử dụng max_completion_tokens ---\")\n",
        "response_recommended = llm_recommended.invoke(messages)\n",
        "print(response_recommended.content)\n",
        "print(f\"Tokens đầu ra: {response_recommended.usage_metadata['output_tokens']}\")\n",
        "\n",
        "print(\"\\n--- Sử dụng max_tokens (được ánh xạ) ---\")\n",
        "response_deprecated = llm_deprecated.invoke(messages)\n",
        "print(response_deprecated.content)\n",
        "print(f\"Tokens đầu ra: {response_deprecated.usage_metadata['output_tokens']}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "GxJQjsQqul9a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "Nếu bạn có bất kỳ câu hỏi nào khác về LangChain, Docker, Portainer hay phát triển full-stack và AI, đừng ngần ngại hỏi nhé\\!"
      ],
      "metadata": {
        "id": "ISK1wBuhul9c"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}